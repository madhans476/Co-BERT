{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["RnSBneT2bonX"],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11504290,"sourceType":"datasetVersion","datasetId":6978346}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Tokenizer","metadata":{"id":"WL67mEBi0jrn"}},{"cell_type":"markdown","source":"### Byte Pair Encoding","metadata":{"id":"Dgv78D1x0oj_"}},{"cell_type":"code","source":"import re\nimport collections\nimport pickle\nfrom typing import Dict, List, Tuple, Set, Optional\nimport numpy as np\n\nclass BPETokenizer:\n    \"\"\"\n    A Byte Pair Encoding tokenizer implementation from scratch.\n    Suitable for code-mixed text like Tamil-English.\n    Enhanced with padding and model preparation capabilities.\n    \"\"\"\n    def __init__(self, vocab_size: int = 10000, max_length: int = 128):\n        \"\"\"\n        Initialize the BPE tokenizer.\n\n        Args:\n            vocab_size: Target vocabulary size (number of merge operations + initial characters)\n            max_length: Default maximum sequence length for padding\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.word_freqs = collections.defaultdict(int)\n        self.vocab = {}\n        self.merges = []\n        self.special_tokens = {\n            \"<PAD>\": 0,\n            \"<UNK>\": 1,\n            \"<BOS>\": 2,\n            \"<EOS>\": 3,\n            \"<SEP>\": 4\n        }\n        # Initialize vocab with special tokens\n        self.vocab = {token: idx for token, idx in self.special_tokens.items()}\n\n    def _preprocess_text(self, text: str) -> List[str]:\n        \"\"\"\n        Preprocesses the text:\n        1. Lowercase the text\n        2. Tokenize into words\n        3. Add spaces before and after each word\n\n        Args:\n            text: Input text\n\n        Returns:\n            List of preprocessed words\n        \"\"\"\n        # You may need to adjust this for Tamil-specific preprocessing\n        text = text.lower()\n\n        # Split by whitespace and punctuation\n        # This regex preserves punctuation as separate tokens\n        words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n\n        # Add word boundary markers\n        words = [f\"▁{word}\" for word in words]\n\n        return words\n\n    def _get_character_level_tokens(self, word: str) -> List[str]:\n        \"\"\"\n        Split a word into character-level tokens.\n\n        Args:\n            word: Input word\n\n        Returns:\n            List of characters\n        \"\"\"\n        return list(word)\n\n    def _get_stats(self, words: List[List[str]]) -> Dict[Tuple[str, str], int]:\n        \"\"\"\n        Count frequency of token pairs across all words.\n\n        Args:\n            words: List of words, where each word is a list of tokens\n\n        Returns:\n            Dictionary mapping token pairs to their frequencies\n        \"\"\"\n        pairs = collections.defaultdict(int)\n        for word in words:\n            for i in range(len(word) - 1):\n                pair = (word[i], word[i + 1])\n                pairs[pair] += 1\n        return pairs\n\n    def _merge_pair(self, words: List[List[str]], pair: Tuple[str, str]) -> List[List[str]]:\n        \"\"\"\n        Merge all occurrences of a token pair in the vocabulary.\n\n        Args:\n            words: List of words, where each word is a list of tokens\n            pair: The pair of tokens to merge\n\n        Returns:\n            Updated list of words with the specified pair merged\n        \"\"\"\n        first, second = pair\n        new_words = []\n\n        for word in words:\n            i = 0\n            new_word = []\n            while i < len(word):\n                if i < len(word) - 1 and word[i] == first and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_words.append(new_word)\n\n        return new_words\n\n    def train(self, corpus: List[str]):\n        \"\"\"\n        Train the BPE tokenizer on a corpus.\n\n        Args:\n            corpus: List of text examples\n        \"\"\"\n        # Preprocess the corpus\n        all_words = []\n        for text in corpus:\n            all_words.extend(self._preprocess_text(text))\n\n        # Count word frequencies\n        for word in all_words:\n            self.word_freqs[word] += 1\n\n        # Split words into characters\n        words = [self._get_character_level_tokens(word) for word in self.word_freqs.keys()]\n\n        # Add all characters to vocabulary\n        unique_chars = set()\n        for word in words:\n            unique_chars.update(word)\n\n        # Add characters to vocabulary\n        for char in sorted(unique_chars):\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n        # Start with the base vocabulary size (special tokens + unique characters)\n        base_vocab_size = len(self.vocab)\n        num_merges = self.vocab_size - base_vocab_size\n        # print(words)\n        # Perform merge operations\n        for i in range(num_merges):\n            # Get pair statistics\n            pairs = self._get_stats(words)\n            if not pairs:\n                break\n\n            # Find the most frequent pair\n            best_pair = max(pairs, key=pairs.get)\n\n            # Merge the pair in all words\n            words = self._merge_pair(words, best_pair)\n\n            # Add the merged token to the vocabulary\n            merged_token = best_pair[0] + best_pair[1]\n            if merged_token not in self.vocab:\n                self.vocab[merged_token] = len(self.vocab)\n\n            # Add the merge operation to the list\n            self.merges.append(best_pair)\n\n            # Print progress\n            if (i + 1) % 100 == 0:\n                print(f\"Merge operation {i+1}/{num_merges}, vocab size: {len(self.vocab)}\")\n\n        # print(\"Merges\")\n        # print(self.merges)\n        # print()\n        # Finalize vocabulary: create reverse mapping\n        self.id_to_token = {idx: token for token, idx in self.vocab.items()}\n\n        print(f\"BPE training complete. Final vocabulary size: {len(self.vocab)}\")\n\n    def tokenize(self, text: str) -> List[int]:\n        \"\"\"\n        Tokenize a text string into token IDs.\n\n        Args:\n            text: Input text\n\n        Returns:\n            List of token IDs\n        \"\"\"\n        # Preprocess the text\n        words = self._preprocess_text(text)\n        token_ids = []\n\n        for word in words:\n            # Start with characters\n            current_tokens = self._get_character_level_tokens(word)\n\n            # Apply merges in the same order as during training\n            for pair in self.merges:\n                current_tokens = self._merge_token_list(current_tokens, pair)\n            # print(f\"current tokens: {word}:: {current_tokens}\")\n            # Convert tokens to IDs\n            for token in current_tokens:\n                if token in self.vocab:\n                    token_ids.append(self.vocab[token])\n                else:\n                    # Handle unknown tokens\n                    token_ids.append(self.special_tokens[\"<UNK>\"])\n\n        return token_ids\n\n    def _merge_token_list(self, tokens: List[str], pair: Tuple[str, str]) -> List[str]:\n        \"\"\"\n        Apply a single merge operation to a list of tokens.\n\n        Args:\n            tokens: List of tokens\n            pair: The pair of tokens to merge\n\n        Returns:\n            Updated list of tokens\n        \"\"\"\n        first, second = pair\n        i = 0\n        result = []\n\n        while i < len(tokens):\n            if i < len(tokens) - 1 and tokens[i] == first and tokens[i + 1] == second:\n                result.append(first + second)\n                i += 2\n            else:\n                result.append(tokens[i])\n                i += 1\n\n        return result\n\n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"\n        Convert token IDs back to text.\n\n        Args:\n            token_ids: List of token IDs\n\n        Returns:\n            Decoded text\n        \"\"\"\n        # Filter out padding tokens\n        token_ids = [idx for idx in token_ids if idx != self.special_tokens[\"<PAD>\"]]\n\n        # Remove special tokens from beginning and end if present\n        if token_ids and token_ids[0] == self.special_tokens[\"<BOS>\"]:\n            token_ids = token_ids[1:]\n        if token_ids and token_ids[-1] == self.special_tokens[\"<EOS>\"]:\n            token_ids = token_ids[:-1]\n\n        tokens = [self.id_to_token.get(idx, \"<UNK>\") for idx in token_ids]\n\n        # Join tokens and remove whitespace marker\n        text = ''.join(tokens)\n        text = text.replace('▁', ' ').strip()\n\n        return text\n\n    def pad_sequences(self, token_ids_list: List[List[int]], max_length: int = None,\n                      padding: str = 'post', truncating: str = 'post') -> List[List[int]]:\n        \"\"\"\n        Pad sequences to the same length.\n\n        Args:\n            token_ids_list: List of token ID sequences\n            max_length: Maximum length to pad to (default: self.max_length or longest sequence)\n            padding: 'pre' or 'post' (where to add padding)\n            truncating: 'pre' or 'post' (where to truncate if needed)\n\n        Returns:\n            List of padded sequences\n        \"\"\"\n        # Find max length if not specified\n        if max_length is None:\n            max_length = self.max_length or max(len(seq) for seq in token_ids_list)\n\n        padded_sequences = []\n        for seq in token_ids_list:\n            # Truncate if necessary\n            if len(seq) > max_length:\n                if truncating == 'pre':\n                    seq = seq[-max_length:]\n                else:  # truncating == 'post'\n                    seq = seq[:max_length]\n\n            # Calculate padding\n            pad_length = max_length - len(seq)\n\n            # Add padding\n            if padding == 'pre':\n                padded_seq = [self.special_tokens['<PAD>']] * pad_length + seq\n            else:  # padding == 'post'\n                padded_seq = seq + [self.special_tokens['<PAD>']] * pad_length\n\n            padded_sequences.append(padded_seq)\n\n        return padded_sequences\n\n    def create_attention_mask(self, padded_sequences: List[List[int]]) -> List[List[int]]:\n        \"\"\"\n        Create attention masks for padded sequences (1 for real tokens, 0 for padding).\n\n        Args:\n            padded_sequences: List of padded token ID sequences\n\n        Returns:\n            List of attention masks\n        \"\"\"\n        masks = []\n        for seq in padded_sequences:\n            mask = [1 if token_id != self.special_tokens['<PAD>'] else 0 for token_id in seq]\n            masks.append(mask)\n        return masks\n\n    def encode_for_model(self, text: str, add_special_tokens: bool = True) -> List[int]:\n        \"\"\"\n        Tokenize text and add special tokens for model input.\n\n        Args:\n            text: Input text\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n\n        Returns:\n            List of token IDs ready for model input\n        \"\"\"\n        token_ids = self.tokenize(text)\n\n        if add_special_tokens:\n            token_ids = [self.special_tokens['<BOS>']] + token_ids + [self.special_tokens['<EOS>']]\n\n        return token_ids\n\n    def prepare_model_inputs(self, texts: List[str], add_special_tokens: bool = True,\n                            max_length: int = None, return_attention_mask: bool = True):\n        \"\"\"\n        Prepare inputs ready for model training or inference.\n\n        Args:\n            texts: List of input texts\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n            max_length: Maximum sequence length (will pad/truncate to this length)\n            return_attention_mask: Whether to return attention masks\n\n        Returns:\n            Dictionary of model inputs\n        \"\"\"\n        # Tokenize all texts\n        all_token_ids = [self.encode_for_model(text, add_special_tokens) for text in texts]\n\n        # Use default max_length if not specified\n        if max_length is None:\n            max_length = self.max_length\n\n        # Pad sequences\n        padded_sequences = self.pad_sequences(all_token_ids, max_length=max_length)\n\n        # Prepare outputs\n        model_inputs = {\n            'input_ids': padded_sequences\n        }\n\n        if return_attention_mask:\n            attention_masks = self.create_attention_mask(padded_sequences)\n            model_inputs['attention_mask'] = attention_masks\n\n        return model_inputs\n\n    def batch_encode(self, texts: List[str], batch_size: int = 32, add_special_tokens: bool = True,\n                    max_length: int = None, return_attention_mask: bool = True):\n        \"\"\"\n        Encode texts and group them into batches ready for model input.\n\n        Args:\n            texts: List of input texts\n            batch_size: Size of each batch\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n            max_length: Maximum sequence length\n            return_attention_mask: Whether to return attention masks\n\n        Returns:\n            List of batched inputs (each a dictionary with 'input_ids' and optionally 'attention_mask')\n        \"\"\"\n        # Prepare all inputs\n        all_inputs = self.prepare_model_inputs(\n            texts,\n            add_special_tokens=add_special_tokens,\n            max_length=max_length,\n            return_attention_mask=return_attention_mask\n        )\n\n        # Create batches\n        batches = []\n        num_samples = len(texts)\n\n        for i in range(0, num_samples, batch_size):\n            batch_indices = slice(i, min(i + batch_size, num_samples))\n            batch = {\n                'input_ids': all_inputs['input_ids'][batch_indices]\n            }\n\n            if return_attention_mask:\n                batch['attention_mask'] = all_inputs['attention_mask'][batch_indices]\n\n            batches.append(batch)\n\n        return batches\n\n    def save(self, path: str):\n        \"\"\"\n        Save the tokenizer to a file.\n\n        Args:\n            path: Path to save the tokenizer\n        \"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump({\n                'vocab': self.vocab,\n                'merges': self.merges,\n                'word_freqs': self.word_freqs,\n                'special_tokens': self.special_tokens,\n                'vocab_size': self.vocab_size,\n                'max_length': self.max_length,\n                'id_to_token': self.id_to_token if hasattr(self, 'id_to_token') else None\n            }, f)\n\n    @classmethod\n    def load(cls, path: str):\n        \"\"\"\n        Load a tokenizer from a file.\n\n        Args:\n            path: Path to the saved tokenizer\n\n        Returns:\n            Loaded BPETokenizer instance\n        \"\"\"\n        with open(path, 'rb') as f:\n            data = pickle.load(f)\n\n        tokenizer = cls(vocab_size=data['vocab_size'], max_length=data.get('max_length', 128))\n        tokenizer.vocab = data['vocab']\n        tokenizer.merges = data['merges']\n        tokenizer.word_freqs = data['word_freqs']\n        tokenizer.special_tokens = data['special_tokens']\n        if data.get('id_to_token'):\n            tokenizer.id_to_token = data['id_to_token']\n        else:\n            tokenizer.id_to_token = {idx: token for token, idx in tokenizer.vocab.items()}\n\n        return tokenizer\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dv4RNQRF0f09","outputId":"93342366-747b-4257-c72f-0ac677f86cb7","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:23.801256Z","iopub.execute_input":"2025-04-22T03:29:23.801618Z","iopub.status.idle":"2025-04-22T03:29:23.868963Z","shell.execute_reply.started":"2025-04-22T03:29:23.801565Z","shell.execute_reply":"2025-04-22T03:29:23.867884Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Wordpiece Tokenizer","metadata":{"id":"t03_FqAuUogN"}},{"cell_type":"code","source":"import re\nimport collections\nimport pickle\nfrom typing import Dict, List, Tuple, Set, Optional\nimport numpy as np\n\nclass WordPieceTokenizer:\n    def __init__(self, vocab_size: int = 10000, max_length: int = 128):\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.word_freqs = collections.defaultdict(int)\n        self.vocab = {}\n        self.merges = []\n        self.special_tokens = {\n            \"<PAD>\": 0,\n            \"<UNK>\": 1,\n            \"<BOS>\": 2,\n            \"<EOS>\": 3,\n            \"<SEP>\": 4,\n            \"<MASK>\": 5\n        }\n\n        # Initialize vocab with special tokens\n        self.vocab = {token: idx for token, idx in self.special_tokens.items()}\n        for i in range(33, 65):\n            char = chr(i)\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n        for i in range(91, 97):\n            char = chr(i)\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n    def _preprocess_text(self, text: str) -> List[str]:\n        text = text.lower()\n        words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n        words = [f\"▁{word}\" for word in words]\n        return words\n\n    def train(self, corpus: List[str]):\n        \"\"\"\n        Train the WordPiece tokenizer on a corpus with strict vocabulary size control.\n        \"\"\"\n        # Preprocess the corpus\n        all_words = []\n        for text in corpus:\n            all_words.extend(self._preprocess_text(text))\n\n        # Count word frequencies\n        for word in all_words:\n            self.word_freqs[word] += 1\n\n        # Generate initial token candidates\n        token_candidates = {}\n\n        # Start with single characters and most frequent whole words\n        for word in self.word_freqs.keys():\n            # Add characters\n            for char in set(word):\n                if char not in token_candidates:\n                    token_candidates[char] = self.word_freqs.get(word, 0)\n\n            # Generate subwords up to 5 characters long\n            for i in range(len(word)):\n                for j in range(i+1, min(i+6, len(word)+1)):\n                    subword = word[i:j]\n                    if len(subword) > 1:\n                        token_candidates[subword] = token_candidates.get(subword, 0) + self.word_freqs.get(word, 0)\n\n        # Sort token candidates by frequency\n        sorted_candidates = sorted(token_candidates.items(), key=lambda x: x[1], reverse=True)\n\n        # Limit vocabulary size, accounting for special tokens\n        max_vocab_tokens = self.vocab_size - len(self.special_tokens)\n        # print(sorted_candidates)\n        # Add top candidates to vocabulary\n        for token, freq in sorted_candidates[:max_vocab_tokens]:\n            if token not in self.vocab:\n                self.vocab[token] = len(self.vocab)\n            if len(self.vocab) == self.vocab_size:\n                break\n\n        # Finalize vocabulary: create reverse mapping\n        self.id_to_token = {idx: token for token, idx in self.vocab.items()}\n\n        print(f\"WordPiece training complete. Final vocabulary size: {len(self.vocab)}\")\n\n    def tokenize(self, text: str) -> List[int]:\n        \"\"\"\n        Tokenize a text string into token IDs using WordPiece algorithm.\n        \"\"\"\n        # Preprocess the text\n        words = self._preprocess_text(text)\n        token_ids = []\n\n        for word in words:\n            # WordPiece tokenization\n            final_tokens = []\n\n            # Start with the full word\n            while word:\n                # Find the longest possible subword\n                found_match = False\n                for length in range(len(word), 0, -1):\n                    subword = word[:length]\n                    if subword in self.vocab:\n                        final_tokens.append(subword)\n                        word = word[length:]\n                        found_match = True\n                        break\n\n                # If no match found, use the first character as UNK\n                if not found_match:\n                    final_tokens.append('<UNK>')\n                    word = word[1:]\n\n            # Convert tokens to IDs\n            for token in final_tokens:\n                if token in self.vocab:\n                    token_ids.append(self.vocab[token])\n                else:\n                    # Handle unknown tokens\n                    token_ids.append(self.special_tokens[\"<UNK>\"])\n\n        return token_ids\n\n\n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"\n        Convert token IDs back to text.\n        Args:\n            token_ids: List of token IDs\n        Returns:\n            Decoded text\n        \"\"\"\n        # Filter out padding tokens\n        token_ids = [idx for idx in token_ids if idx != self.special_tokens[\"<PAD>\"]]\n\n        # Remove special tokens from beginning and end if present\n        if token_ids and token_ids[0] == self.special_tokens[\"<BOS>\"]:\n            token_ids = token_ids[1:]\n        if token_ids and token_ids[-1] == self.special_tokens[\"<EOS>\"]:\n            token_ids = token_ids[:-1]\n\n        tokens = [self.id_to_token.get(idx, \"<UNK>\") for idx in token_ids]\n\n        # Join tokens and remove whitespace marker\n        text = ''.join(tokens)\n        text = text.replace('▁', '').strip()\n        return text\n\n    def pad_sequences(self, token_ids_list: List[List[int]], max_length: int = None,\n                      padding: str = 'post', truncating: str = 'post') -> List[List[int]]:\n        \"\"\"\n        Pad sequences to the same length.\n\n        Args:\n            token_ids_list: List of token ID sequences\n            max_length: Maximum length to pad to (default: self.max_length or longest sequence)\n            padding: 'pre' or 'post' (where to add padding)\n            truncating: 'pre' or 'post' (where to truncate if needed)\n\n        Returns:\n            List of padded sequences\n        \"\"\"\n        # Find max length if not specified\n        if max_length is None:\n            max_length = self.max_length or max(len(seq) for seq in token_ids_list)\n\n        padded_sequences = []\n        for seq in token_ids_list:\n            # Truncate if necessary\n            if len(seq) > max_length:\n                if truncating == 'pre':\n                    seq = seq[-max_length:]\n                else:  # truncating == 'post'\n                    seq = seq[:max_length]\n\n            # Calculate padding\n            pad_length = max_length - len(seq)\n\n            # Add padding\n            if padding == 'pre':\n                padded_seq = [self.special_tokens['<PAD>']] * pad_length + seq\n            else:  # padding == 'post'\n                padded_seq = seq + [self.special_tokens['<PAD>']] * pad_length\n\n            padded_sequences.append(padded_seq)\n\n        return padded_sequences\n\n    def create_attention_mask(self, padded_sequences: List[List[int]]) -> List[List[int]]:\n        \"\"\"\n        Create attention masks for padded sequences (1 for real tokens, 0 for padding).\n\n        Args:\n            padded_sequences: List of padded token ID sequences\n\n        Returns:\n            List of attention masks\n        \"\"\"\n        masks = []\n        for seq in padded_sequences:\n            mask = [1 if token_id != self.special_tokens['<PAD>'] else 0 for token_id in seq]\n            masks.append(mask)\n        return masks\n\n    def encode_for_model(self, text: str, add_special_tokens: bool = True) -> List[int]:\n        \"\"\"\n        Tokenize text and add special tokens for model input.\n\n        Args:\n            text: Input text\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n\n        Returns:\n            List of token IDs ready for model input\n        \"\"\"\n        token_ids = self.tokenize(text)\n\n        if add_special_tokens:\n            token_ids = [self.special_tokens['<BOS>']] + token_ids + [self.special_tokens['<EOS>']]\n\n        return token_ids\n\n    def prepare_model_inputs(self, texts: List[str], add_special_tokens: bool = True,\n                            max_length: int = None, return_attention_mask: bool = True):\n        \"\"\"\n        Prepare inputs ready for model training or inference.\n\n        Args:\n            texts: List of input texts\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n            max_length: Maximum sequence length (will pad/truncate to this length)\n            return_attention_mask: Whether to return attention masks\n\n        Returns:\n            Dictionary of model inputs\n        \"\"\"\n        # Tokenize all texts\n        all_token_ids = [self.encode_for_model(text, add_special_tokens) for text in texts]\n\n        # Use default max_length if not specified\n        if max_length is None:\n            max_length = self.max_length\n\n        # Pad sequences\n        padded_sequences = self.pad_sequences(all_token_ids, max_length=max_length)\n\n        # Prepare outputs\n        model_inputs = {\n            'input_ids': padded_sequences\n        }\n\n        if return_attention_mask:\n            attention_masks = self.create_attention_mask(padded_sequences)\n            model_inputs['attention_mask'] = attention_masks\n\n        return model_inputs\n\n    def save(self, path: str):\n        \"\"\"\n        Save the tokenizer to a file.\n        Args:\n            path: Path to save the tokenizer\n        \"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump({\n                'vocab': self.vocab,\n                'word_freqs': self.word_freqs,\n                'special_tokens': self.special_tokens,\n                'vocab_size': self.vocab_size,\n                'max_length': self.max_length,\n                'id_to_token': self.id_to_token\n            }, f)\n\n    @classmethod\n    def load(cls, path: str):\n        \"\"\"\n        Load a tokenizer from a file.\n        Args:\n            path: Path to the saved tokenizer\n        Returns:\n            Loaded WordPieceTokenizer instance\n        \"\"\"\n        with open(path, 'rb') as f:\n            data = pickle.load(f)\n\n        tokenizer = cls(vocab_size=data['vocab_size'], max_length=data.get('max_length', 128))\n        tokenizer.vocab = data['vocab']\n        tokenizer.word_freqs = data['word_freqs']\n        tokenizer.special_tokens = data['special_tokens']\n        tokenizer.id_to_token = data['id_to_token']\n\n        return tokenizer","metadata":{"id":"ay_TL-G_Uv9n","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:23.870251Z","iopub.execute_input":"2025-04-22T03:29:23.870556Z","iopub.status.idle":"2025-04-22T03:29:23.895464Z","shell.execute_reply.started":"2025-04-22T03:29:23.870533Z","shell.execute_reply":"2025-04-22T03:29:23.894722Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Sentence piece tokenizer","metadata":{"id":"GOsdEW0lubtC"}},{"cell_type":"code","source":"import re\nimport collections\nimport pickle\nfrom typing import Dict, List, Tuple, Set, Optional\nimport math\n\nclass SentencePieceTokenizer:\n    \"\"\"\n    A SentencePiece tokenizer implementation from scratch.\n    Supports unigram language model tokenization with subword units.\n    \"\"\"\n    def __init__(self, vocab_size: int = 10000, max_length: int = 128,\n                 max_piece_length: int = 16, alpha: float = 0.1):\n        \"\"\"\n        Initialize the SentencePiece tokenizer.\n\n        Args:\n            vocab_size: Target vocabulary size\n            max_length: Maximum sequence length for padding\n            max_piece_length: Maximum length of a subword piece\n            alpha: Smoothing parameter for unigram language model\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.max_piece_length = max_piece_length\n        self.alpha = alpha\n\n        # Special tokens\n        self.special_tokens = {\n            \"<PAD>\": 0,\n            \"<UNK>\": 1,\n            \"<BOS>\": 2,\n            \"<EOS>\": 3,\n            \"<SEP>\": 4\n        }\n\n        # Additional special token mappings for printable ASCII\n        for i in range(33, 65):\n            self.special_tokens[chr(i)] = i - 27\n        for i in range(91, 127):\n            self.special_tokens[chr(i)] = i - 53\n\n        self.vocab = {token: idx for token, idx in self.special_tokens.items()}\n        self.token_frequencies = {}\n        self.vocab_scores = {}\n\n    def _preprocess_text(self, text: str) -> str:\n        \"\"\"\n        Preprocess input text.\n\n        Args:\n            text: Input text\n        Returns:\n            Preprocessed text\n        \"\"\"\n        # Lowercase and normalize\n        text = text.lower()\n        # Add word boundary markers\n        text = '▁' + text.replace(' ', ' ▁')\n        return text\n\n    def _enumerate_patterns(self, text: str) -> List[str]:\n        \"\"\"\n        Generate all possible subword pieces of a text.\n\n        Args:\n            text: Input text\n        Returns:\n            List of all possible subword pieces\n        \"\"\"\n        pieces = []\n        for length in range(1, min(len(text) + 1, self.max_piece_length + 1)):\n            for start in range(len(text) - length + 1):\n                pieces.append(text[start:start+length])\n        return pieces\n\n    def _compute_unigram_loss(self, vocab: Dict[str, int]) -> float:\n        \"\"\"\n        Compute the unigram language model loss.\n\n        Args:\n            vocab: Current vocabulary\n        Returns:\n            Total loss of the unigram model\n        \"\"\"\n        total_loss = 0\n        for text, freq in self.token_frequencies.items():\n            piece_loss = float('inf')\n            for piece_length in range(1, len(text) + 1):\n                current_loss = 0\n                for start in range(0, len(text), piece_length):\n                    end = start + piece_length\n                    if end > len(text):\n                        break\n                    piece = text[start:end]\n                    if piece in vocab:\n                        current_loss -= math.log(self.vocab_scores.get(piece, 1))\n                    else:\n                        # Unknown piece penalty\n                        current_loss += 10\n                piece_loss = min(piece_loss, current_loss)\n            total_loss += freq * piece_loss\n        return total_loss\n\n    def train(self, corpus: List[str]):\n        \"\"\"\n        Train the SentencePiece tokenizer.\n\n        Args:\n            corpus: List of text examples\n        \"\"\"\n        # Preprocess and count frequencies\n        preprocessed_corpus = [self._preprocess_text(text) for text in corpus]\n\n        # Generate initial set of candidates\n        candidates = set()\n        for text in preprocessed_corpus:\n            candidates.update(self._enumerate_patterns(text))\n\n        # Initial frequency estimation\n        token_freqs = collections.defaultdict(int)\n        for text in preprocessed_corpus:\n            for candidate in candidates:\n                token_freqs[candidate] += text.count(candidate)\n\n        # Sort candidates by frequency\n        sorted_candidates = sorted(token_freqs.items(), key=lambda x: x[1], reverse=True)\n\n        # Initialize vocabulary with most frequent tokens\n        vocab = {token: idx for idx, (token, _) in enumerate(sorted_candidates[:self.vocab_size],\n                                                             start=len(self.special_tokens))}\n\n        # Iterative refinement with unigram loss minimization\n        iterations = 10\n        for _ in range(iterations):\n            # Update token frequencies and scores\n            self.token_frequencies = {k: v for k, v in token_freqs.items() if k in vocab}\n\n            # Compute token scores (log probability)\n            total_tokens = sum(self.token_frequencies.values())\n            self.vocab_scores = {\n                token: math.log((freq + self.alpha) / (total_tokens + self.alpha * len(vocab)))\n                for token, freq in self.token_frequencies.items()\n            }\n\n            # Prune vocabulary based on loss\n            new_vocab = {}\n            candidates = sorted(\n                [(token, score) for token, score in self.vocab_scores.items()],\n                key=lambda x: x[1]\n            )\n\n            for token, idx in self.special_tokens.items():\n                new_vocab[token] = idx\n\n            # Keep special tokens and top vocabulary\n            for token, _ in candidates[:self.vocab_size]:\n                if token not in new_vocab:\n                    new_vocab[token] = len(new_vocab)\n                if len(new_vocab) == self.vocab_size:\n                    break\n\n            # Merge special tokens with new vocabulary\n            \n\n            vocab = new_vocab\n\n        # Finalize vocabulary\n        self.vocab = vocab\n        self.id_to_token = {idx: token for token, idx in self.vocab.items()}\n\n        print(f\"SentencePiece training complete. Final vocabulary size: {len(self.vocab)}\")\n\n    def tokenize(self, text: str) -> List[int]:\n        \"\"\"\n        Tokenize text into token IDs.\n\n        Args:\n            text: Input text\n        Returns:\n            List of token IDs\n        \"\"\"\n        # Preprocess text\n        text = self._preprocess_text(text)\n\n        # Greedy longest match tokenization\n        token_ids = []\n        while text:\n            best_piece = None\n            for length in range(min(len(text), self.max_piece_length), 0, -1):\n                piece = text[:length]\n                if piece in self.vocab:\n                    best_piece = piece\n                    break\n\n            if best_piece is None:\n                # Unknown token\n                token_ids.append(self.special_tokens[\"<UNK>\"])\n                text = text[1:]\n            else:\n                token_ids.append(self.vocab[best_piece])\n                text = text[len(best_piece):]\n\n        return token_ids\n\n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"\n        Convert token IDs back to text.\n\n        Args:\n            token_ids: List of token IDs\n        Returns:\n            Decoded text\n        \"\"\"\n        # Filter out padding tokens\n        token_ids = [idx for idx in token_ids if idx != self.special_tokens[\"<PAD>\"]]\n\n        # Remove special tokens from beginning and end if present\n        if token_ids and token_ids[0] == self.special_tokens[\"<BOS>\"]:\n            token_ids = token_ids[1:]\n        if token_ids and token_ids[-1] == self.special_tokens[\"<EOS>\"]:\n            token_ids = token_ids[:-1]\n\n        # Convert to tokens\n        tokens = [self.id_to_token.get(idx, \"<UNK>\") for idx in token_ids]\n\n        # Reconstruct text with proper handling of word boundary markers\n        text = ''.join(tokens)\n        text = text.replace('▁', '').strip()\n\n        return text\n\n\n    def pad_sequences(self, token_ids_list: List[List[int]], max_length: int = None,\n                     padding: str = 'post', truncating: str = 'post') -> List[List[int]]:\n        \"\"\"\n        Pad sequences to the same length.\n\n        Args:\n            token_ids_list: List of token ID sequences\n            max_length: Maximum length to pad to\n            padding: 'pre' or 'post'\n            truncating: 'pre' or 'post'\n        Returns:\n            List of padded sequences\n        \"\"\"\n        # Find max length if not specified\n        if max_length is None:\n            max_length = self.max_length or max(len(seq) for seq in token_ids_list)\n\n        padded_sequences = []\n        for seq in token_ids_list:\n            # Truncate if necessary\n            if len(seq) > max_length:\n                seq = seq[:max_length] if truncating == 'post' else seq[-max_length:]\n\n            # Pad if necessary\n            pad_length = max_length - len(seq)\n            if padding == 'pre':\n                padded_seq = [self.special_tokens['<PAD>']] * pad_length + seq\n            else:  # padding == 'post'\n                padded_seq = seq + [self.special_tokens['<PAD>']] * pad_length\n\n            padded_sequences.append(padded_seq)\n\n        return padded_sequences\n\n    def create_attention_mask(self, padded_sequences: List[List[int]]) -> List[List[int]]:\n        \"\"\"\n        Create attention masks for padded sequences.\n\n        Args:\n            padded_sequences: List of padded token ID sequences\n        Returns:\n            List of attention masks\n        \"\"\"\n        masks = []\n        for seq in padded_sequences:\n            mask = [1 if token_id != self.special_tokens['<PAD>'] else 0 for token_id in seq]\n            masks.append(mask)\n        return masks\n\n    def encode_for_model(self, text: str, add_special_tokens: bool = True) -> List[int]:\n        \"\"\"\n        Tokenize text and add special tokens for model input.\n\n        Args:\n            text: Input text\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n        Returns:\n            List of token IDs ready for model input\n        \"\"\"\n        token_ids = self.tokenize(text)\n        if add_special_tokens:\n            token_ids = [self.special_tokens['<BOS>']] + token_ids + [self.special_tokens['<EOS>']]\n        return token_ids\n\n    def prepare_model_inputs(self, texts: List[str], add_special_tokens: bool = True,\n                            max_length: int = None, return_attention_mask: bool = True):\n        \"\"\"\n        Prepare inputs ready for model training or inference.\n\n        Args:\n            texts: List of input texts\n            add_special_tokens: Whether to add <BOS> and <EOS> tokens\n            max_length: Maximum sequence length (will pad/truncate to this length)\n            return_attention_mask: Whether to return attention masks\n\n        Returns:\n            Dictionary of model inputs\n        \"\"\"\n        # Tokenize all texts\n        all_token_ids = [self.encode_for_model(text, add_special_tokens) for text in texts]\n\n        # Use default max_length if not specified\n        if max_length is None:\n            max_length = self.max_length\n\n        # Pad sequences\n        padded_sequences = self.pad_sequences(all_token_ids, max_length=max_length)\n\n        # Prepare outputs\n        model_inputs = {\n            'input_ids': padded_sequences\n        }\n\n        if return_attention_mask:\n            attention_masks = self.create_attention_mask(padded_sequences)\n            model_inputs['attention_mask'] = attention_masks\n\n        return model_inputs\n\n    def save(self, path: str):\n        \"\"\"\n        Save the tokenizer to a file.\n\n        Args:\n            path: Path to save the tokenizer\n        \"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump({\n                'vocab': self.vocab,\n                'token_frequencies': self.token_frequencies,\n                'vocab_scores': self.vocab_scores,\n                'special_tokens': self.special_tokens,\n                'vocab_size': self.vocab_size,\n                'max_length': self.max_length,\n                'id_to_token': self.id_to_token\n            }, f)\n\n    @classmethod\n    def load(cls, path: str):\n        \"\"\"\n        Load a tokenizer from a file.\n\n        Args:\n            path: Path to the saved tokenizer\n        Returns:\n            Loaded SentencePieceTokenizer instance\n        \"\"\"\n        with open(path, 'rb') as f:\n            data = pickle.load(f)\n\n        tokenizer = cls(vocab_size=data['vocab_size'], max_length=data.get('max_length', 128))\n        tokenizer.vocab = data['vocab']\n        tokenizer.token_frequencies = data['token_frequencies']\n        tokenizer.vocab_scores = data['vocab_scores']\n        tokenizer.special_tokens = data['special_tokens']\n        tokenizer.id_to_token = data['id_to_token']\n\n        return tokenizer\n","metadata":{"id":"O_UItOBguf4N","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:23.954922Z","iopub.execute_input":"2025-04-22T03:29:23.955270Z","iopub.status.idle":"2025-04-22T03:29:23.981620Z","shell.execute_reply.started":"2025-04-22T03:29:23.955227Z","shell.execute_reply":"2025-04-22T03:29:23.980935Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Co-BERT","metadata":{"id":"RnSBneT2bonX"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\n# from transformers import BertTokenizer\n# from datasets import load_dataset\nimport math\nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# # Load a smaller dataset - just a portion of Wikipedia\n# print(\"Loading dataset...\")\n# wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:5%]\")\n# wiki_dataset = wiki_dataset.remove_columns([col for col in wiki_dataset.column_names if col != \"text\"])\n# print(f\"Dataset size: {len(wiki_dataset)} documents\")\n\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n\nclass BertConfig:\n    def __init__(self,\n                vocab_size=30522,\n                hidden_size=768,\n                num_hidden_layers=6,  # Reduced\n                num_attention_heads=12,\n                intermediate_size=3072,\n                hidden_dropout_prob=0.1,\n                attention_probs_dropout_prob=0.1,\n                max_position_embeddings=512,\n                type_vocab_size=2,\n                initializer_range=0.02,\n                layer_norm_eps=1e-12):\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n\n# Multi-Head Attention\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                f\"Hidden size ({config.hidden_size}) not divisible by number of attention heads ({config.num_attention_heads})\")\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        # Create query, key, value projections\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        \"\"\"Reshape to separate multiple heads\"\"\"\n        batch_size, seq_length, _ = x.size()\n        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n        return x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, head_size)\n\n    def forward(self, hidden_states, attention_mask=None):\n\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Calculate attention scores\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\n        # Apply attention mask if provided\n        if attention_mask is not None:\n            # Mask has shape [batch_size, 1, 1, seq_length]\n            attention_scores = attention_scores + attention_mask\n\n        attention_probs = F.softmax(attention_scores, dim=-1)\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        batch_size, seq_length, _, _ = context_layer.size()\n        context_layer = context_layer.view(batch_size, seq_length, self.all_head_size)\n\n        return context_layer\n\n# Output projection after self-attention\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n# Complete attention block\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self_attention = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, hidden_states, attention_mask=None):\n        self_outputs = self.self_attention(hidden_states, attention_mask)\n        attention_output = self.output(self_outputs, hidden_states)\n        return attention_output\n\n# Feed-forward network\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.gelu = nn.GELU()\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states\n\n# Output layer after feed-forward\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n# Complete encoder layer\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask=None):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n# Stack of encoder layers\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        self.layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask=None):\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n        return hidden_states\n\n# Embeddings (token, position, segment)\nclass BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # Position IDs (0, 1, 2, ..., max_len)\n        self.register_buffer(\n            \"position_ids\",\n            torch.arange(config.max_position_embeddings).expand((1, -1))\n        )\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, :seq_length]\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n\n        return embeddings\n\n# Masked Language Model head\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(BertLMPredictionHead, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.GELU()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.activation(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n# Next Sentence Prediction head\nclass BertNextSentenceHead(nn.Module):\n    def __init__(self, config):\n        super(BertNextSentenceHead, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, 2)  # Binary classification: IsNext or NotNext\n\n    def forward(self, pooled_output):\n        return self.dense(pooled_output)\n\n# Pooler layer for sentence-level tasks\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # Take hidden state of [CLS] token\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n# Complete BERT model\nclass BertModel(nn.Module):\n    def __init__(self, config):\n        super(BertModel, self).__init__()\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n\n        # Reshape attention mask for broadcast to attention heads\n        # [batch_size, seq_length] -> [batch_size, 1, 1, seq_length]\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Convert mask values: 0 -> -10000, 1 -> 0\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Get embeddings\n        embedding_output = self.embeddings(input_ids, token_type_ids, position_ids)\n\n        # Pass through encoder layers\n        sequence_output = self.encoder(embedding_output, extended_attention_mask)\n\n        # Apply pooling for sentence representation\n        pooled_output = self.pooler(sequence_output)\n\n        return sequence_output, pooled_output\n\n# BERT for pre-training (combines MLM and NSP tasks)\nclass BertForPreTraining(nn.Module):\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__()\n        self.bert = BertModel(config)\n        self.cls = nn.ModuleDict({\n            'predictions': BertLMPredictionHead(config),\n            'seq_relationship': BertNextSentenceHead(config)\n        })\n\n        # Initialize weights\n        self.apply(self._init_weights)\n\n        # Tie input and output embeddings\n        self.cls['predictions'].decoder.weight = self.bert.embeddings.word_embeddings.weight\n\n    def _init_weights(self, module):\n        \"\"\"Initialize weights for linear layers and embeddings\"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(\n            input_ids,\n            token_type_ids,\n            attention_mask\n        )\n\n        # MLM prediction scores\n        prediction_scores = self.cls['predictions'](sequence_output)\n\n        # NSP prediction scores\n        seq_relationship_score = self.cls['seq_relationship'](pooled_output)\n\n        # Calculate loss if labels are provided\n        outputs = (prediction_scores, seq_relationship_score)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            # MLM loss (CrossEntropyLoss)\n            mlm_loss = F.cross_entropy(\n                prediction_scores.view(-1, prediction_scores.size(-1)),\n                masked_lm_labels.view(-1),\n                ignore_index=-100  # Ignore padding tokens\n            )\n\n            # NSP loss (CrossEntropyLoss)\n            nsp_loss = F.cross_entropy(\n                seq_relationship_score.view(-1, 2),\n                next_sentence_label.view(-1)\n            )\n\n            total_loss = mlm_loss + nsp_loss\n            outputs = (total_loss,) + outputs\n\n        return outputs\n\n# Dataset class for BERT pre-training\nimport torch\nfrom torch.utils.data import Dataset\nimport random\n\nclass BertPretrainingDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=128, mlm_probability=0.15):\n        \"\"\"\n        Initialize the BERT pretraining dataset with a custom BPE tokenizer.\n\n        Args:\n            texts (List[str]): List of input texts\n            tokenizer (BPETokenizer): Custom BPE tokenizer\n            max_length (int): Maximum sequence length\n            mlm_probability (float): Probability of masking tokens\n        \"\"\"\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.mlm_probability = mlm_probability\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n\n        # Split text into sentences\n        sentences = text.split('. ')\n        if len(sentences) < 2:\n            # print(\"YES\")\n            # If there's only one sentence, duplicate it\n            sentences.append(sentences[0])\n\n        # Create sentence pairs for NSP (50% are actual next sentences)\n        is_next = random.choice([True, False])\n        if is_next:\n            # Actual next sentence\n            first_idx = random.randint(0, len(sentences) - 2)\n            second_idx = first_idx + 1\n        else:\n            # Random sentence (not next)\n            first_idx = random.randint(0, len(sentences) - 1)\n            second_idx = random.randint(0, len(sentences) - 1)\n            while second_idx == first_idx + 1:  # Ensure it's not actually the next sentence\n                second_idx = random.randint(0, len(sentences) - 1)\n\n        # Get the sentences\n        sentence_a = sentences[first_idx]\n        sentence_b = sentences[second_idx]\n\n        # Combine sentences similar to BERT's [CLS] sentence_a [SEP] sentence_b [SEP]\n        combined_text = f\"{sentence_a} {self.tokenizer.id_to_token[self.tokenizer.special_tokens['<SEP>']]} {sentence_b}\"\n\n        # Prepare model inputs using custom BPE tokenizer\n        model_inputs = self.tokenizer.prepare_model_inputs(\n            [combined_text],\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_attention_mask=True\n        )\n\n        # Convert to PyTorch tensors\n        input_ids = torch.tensor(model_inputs['input_ids'][0], dtype=torch.long)\n        attention_mask = torch.tensor(model_inputs['attention_mask'][0], dtype=torch.long)\n\n        # Create token type ids (0 for first sentence, 1 for second sentence)\n        token_type_ids = torch.zeros_like(input_ids, dtype=torch.long)\n        sep_token_id = self.tokenizer.special_tokens['<SEP>']\n        sep_indices = torch.where(input_ids == sep_token_id)[0]\n        if len(sep_indices) > 0:\n            token_type_ids[sep_indices[0]+1:] = 1\n\n        # Create masked LM labels\n        mlm_labels = input_ids.clone()\n\n        # Create a mask for special tokens\n        special_tokens_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n        special_token_ids = set([\n            self.tokenizer.special_tokens['<PAD>'],\n            self.tokenizer.special_tokens['<UNK>'],\n            self.tokenizer.special_tokens['<BOS>'],\n            self.tokenizer.special_tokens['<EOS>'],\n            self.tokenizer.special_tokens.get('<SEP>', -1)\n        ])\n        for special_id in special_token_ids:\n            special_tokens_mask |= (input_ids == special_id)\n\n        # Get probability mask for tokens to predict (15% of non-special tokens)\n        probability_matrix = torch.full(input_ids.shape, self.mlm_probability)\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n\n        # Get indices of tokens to mask\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        mlm_labels[~masked_indices] = -100  # -100 index will be ignored in loss\n\n        # Mask tokens (80% of selected tokens)\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.vocab[\"<UNK>\"]\n\n        # Replace with random words (10% of selected tokens)\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer.vocab), input_ids.shape, dtype=torch.long)\n        input_ids[indices_random] = random_words[indices_random]\n\n        # NSP label: 1 for IsNext, 0 for NotNext\n        nsp_label = 1 if is_next else 0\n\n        return {\n            'input_ids': input_ids,\n            'token_type_ids': token_type_ids,\n            'attention_mask': attention_mask,\n            'masked_lm_labels': mlm_labels,\n            'next_sentence_label': torch.tensor(nsp_label, dtype=torch.long)\n        }\n\n\n# Preprocess the dataset\ndef preprocess_dataset(dataset):\n    # Filter out very short documents\n    filtered_dataset = [doc for doc in dataset if len(doc.split()) > 50]\n    return filtered_dataset\n\n# Training loop\ndef train():\n    model.train()\n    epoch_loss = 0\n\n    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n    for batch in progress_bar:\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(\n            input_ids=batch['input_ids'],\n            token_type_ids=batch['token_type_ids'],\n            attention_mask=batch['attention_mask'],\n            masked_lm_labels=batch['masked_lm_labels'],\n            next_sentence_label=batch['next_sentence_label']\n        )\n\n        loss = outputs[0]\n\n        # Backward pass\n        loss.backward()\n\n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update weights\n        optimizer.step()\n        scheduler.step()\n\n        # Update progress bar\n        progress_bar.set_postfix({'loss': loss.item()})\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(train_dataloader)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hc9oHV73boAT","outputId":"dd077167-2c2e-4bb8-aaa7-6c92ef93ea46","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:23.982672Z","iopub.execute_input":"2025-04-22T03:29:23.983012Z","iopub.status.idle":"2025-04-22T03:29:27.919147Z","shell.execute_reply.started":"2025-04-22T03:29:23.982976Z","shell.execute_reply":"2025-04-22T03:29:27.918226Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"LMVx_JeLfw6C"}},{"cell_type":"code","source":"import re\n\ndef clean_sentences(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    text = text.replace('\\ufeff', '')\n    text = text.replace('\\n', ' ')\n    sentences = re.split(r'(?<=\\.)\\s+', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n\n    return sentences\n\ndef count_unique_words(sentences):\n    unique_words = set()\n\n    for sentence in sentences:\n        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n        unique_words.update(words)\n\n    return len(unique_words)\n","metadata":{"id":"NBeF-6xwd4Sb","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:27.920581Z","iopub.execute_input":"2025-04-22T03:29:27.920952Z","iopub.status.idle":"2025-04-22T03:29:27.925957Z","shell.execute_reply.started":"2025-04-22T03:29:27.920931Z","shell.execute_reply":"2025-04-22T03:29:27.925087Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tanglish_path = '/kaggle/input/code-mix-dataset-for-pretraining/tanglish.txt'\ntanglish = clean_sentences(tanglish_path)\nprint(f\"Number of unique words in the corpus: {count_unique_words(tanglish)}\")\nct = len(tanglish)\nprint(f\"Total number of sentences: {ct}\")\ntamil_corpus = tanglish[:int(ct*0.8)]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGyqPRCId3_W","outputId":"2f492d43-78b8-4119-b6ca-d0f8003cc9f1","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:27.927078Z","iopub.execute_input":"2025-04-22T03:29:27.927302Z","iopub.status.idle":"2025-04-22T03:29:27.986989Z","shell.execute_reply.started":"2025-04-22T03:29:27.927283Z","shell.execute_reply":"2025-04-22T03:29:27.986112Z"}},"outputs":[{"name":"stdout","text":"Number of unique words in the corpus: 5995\nTotal number of sentences: 1581\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"kanglish_path = '/kaggle/input/code-mix-dataset-for-pretraining/Kanglish.txt'\nkanglish = clean_sentences(kanglish_path)\nprint(f\"Number of unique words in the corpus: {count_unique_words(kanglish)}\")\nct = len(kanglish)\nprint(f\"Total number of sentences: {ct}\")\nkannada_corpus = kanglish[:int(ct*0.8)]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8pwOZEPd373","outputId":"9b935b80-2a36-4086-9897-e45ea58373ba","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:27.987925Z","iopub.execute_input":"2025-04-22T03:29:27.988246Z","iopub.status.idle":"2025-04-22T03:29:28.014498Z","shell.execute_reply.started":"2025-04-22T03:29:27.988215Z","shell.execute_reply":"2025-04-22T03:29:28.013845Z"}},"outputs":[{"name":"stdout","text":"Number of unique words in the corpus: 5867\nTotal number of sentences: 1722\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"hinglish_path = '/kaggle/input/code-mix-dataset-for-pretraining/english based hinglish.txt'\nhinglish = clean_sentences(hinglish_path)\nprint(f\"Number of unique words in the corpus: {count_unique_words(hinglish)}\")\nct = len(hinglish)\nprint(f\"Total number of sentences: {ct}\")\nhindi_corpus = hinglish[:int(ct*0.8)]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLuFKybWd30B","outputId":"6b12e4f2-5f3a-42e2-eda3-531d9dd66171","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:28.015214Z","iopub.execute_input":"2025-04-22T03:29:28.015413Z","iopub.status.idle":"2025-04-22T03:29:28.052186Z","shell.execute_reply.started":"2025-04-22T03:29:28.015395Z","shell.execute_reply":"2025-04-22T03:29:28.051529Z"}},"outputs":[{"name":"stdout","text":"Number of unique words in the corpus: 5846\nTotal number of sentences: 1736\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"id":"ixlhqTozd3tP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BPE instances","metadata":{"id":"ab-iu8gCicnM"}},{"cell_type":"code","source":"# Initialize the custom BPE tokenizer\nbpe_tam_tokenizer = BPETokenizer(vocab_size=2500, max_length=128)\nbpe_tam_tokenizer.train(tamil_corpus)\n","metadata":{"id":"SYDPZYOCiiIe","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:29:28.052957Z","iopub.execute_input":"2025-04-22T03:29:28.053161Z","iopub.status.idle":"2025-04-22T03:30:11.646672Z","shell.execute_reply.started":"2025-04-22T03:29:28.053140Z","shell.execute_reply":"2025-04-22T03:30:11.645881Z"}},"outputs":[{"name":"stdout","text":"Merge operation 100/2448, vocab size: 152\nMerge operation 200/2448, vocab size: 252\nMerge operation 300/2448, vocab size: 352\nMerge operation 400/2448, vocab size: 452\nMerge operation 500/2448, vocab size: 552\nMerge operation 600/2448, vocab size: 652\nMerge operation 700/2448, vocab size: 752\nMerge operation 800/2448, vocab size: 852\nMerge operation 900/2448, vocab size: 952\nMerge operation 1000/2448, vocab size: 1052\nMerge operation 1100/2448, vocab size: 1152\nMerge operation 1200/2448, vocab size: 1252\nMerge operation 1300/2448, vocab size: 1352\nMerge operation 1400/2448, vocab size: 1452\nMerge operation 1500/2448, vocab size: 1552\nMerge operation 1600/2448, vocab size: 1652\nMerge operation 1700/2448, vocab size: 1752\nMerge operation 1800/2448, vocab size: 1852\nMerge operation 1900/2448, vocab size: 1952\nMerge operation 2000/2448, vocab size: 2052\nMerge operation 2100/2448, vocab size: 2152\nMerge operation 2200/2448, vocab size: 2252\nMerge operation 2300/2448, vocab size: 2352\nMerge operation 2400/2448, vocab size: 2452\nBPE training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"bert_tam_dataset = BertPretrainingDataset(\n    texts=tanglish,\n    tokenizer=bpe_tam_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_tam_dataset)\ntrain_dataloader = DataLoader(\n    bert_tam_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3   # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\n# Save the model\n# model_save_path = \"bert_pretrained_small.pt\"\n# torch.save(model.state_dict(), model_save_path)\n# print(f\"Model saved to {model_save_path}\")\n\nprint(\"Training complete!\")","metadata":{"id":"wxYDNitZkHvu","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:20:17.115630Z","iopub.execute_input":"2025-04-22T05:20:17.115992Z","iopub.status.idle":"2025-04-22T05:22:40.370253Z","shell.execute_reply.started":"2025-04-22T05:20:17.115964Z","shell.execute_reply":"2025-04-22T05:22:40.369223Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:46<00:00,  2.11it/s, loss=8.03]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.1090\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:48<00:00,  2.06it/s, loss=7.55]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.4071\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:46<00:00,  2.12it/s, loss=7.38]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.2053\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"bert_tam_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:31:52.612290Z","iopub.execute_input":"2025-04-22T03:31:52.612828Z","iopub.status.idle":"2025-04-22T03:31:52.695734Z","shell.execute_reply.started":"2025-04-22T03:31:52.612805Z","shell.execute_reply":"2025-04-22T03:31:52.695148Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([   2,  128,   80,   42, 1626,  920,  152,  185, 1627,  126,  318,    1,\n         1628, 1181,  770, 1182,  107, 2204, 1183,  597,  240, 1629, 1630, 1631,\n          771, 1632,    1,  112,   51,   10,   51,    1,  197,   38,   51,    1,\n          128,   80,   42, 1626,  920,  152,    1, 1627,    1,    1, 1180, 1628,\n         1181,  770, 1182,  107,  433, 1183,  597,    1, 1629, 1630,    1, 1351,\n         1632,    1,  112,   51,   10,    3,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'masked_lm_labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1180,\n         -100, -100, -100, -100, -100,  433, -100, -100, -100, -100, -100, -100,\n         -100, -100, 1633, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100,  185, -100,  126,  318, -100, 1628,\n         -100, -100, -100, -100, -100, -100, -100,  240, -100, -100, 1631,  771,\n         -100, 1633, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100]),\n 'next_sentence_label': tensor(1)}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Initialize the custom BPE tokenizer\nbpe_kan_tokenizer = BPETokenizer(vocab_size=2500, max_length=128)\nbpe_kan_tokenizer.train(kannada_corpus)\n","metadata":{"id":"zzn0Fns9i7sX","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:31:52.696799Z","iopub.execute_input":"2025-04-22T03:31:52.697102Z","iopub.status.idle":"2025-04-22T03:32:36.079416Z","shell.execute_reply.started":"2025-04-22T03:31:52.697071Z","shell.execute_reply":"2025-04-22T03:32:36.078642Z"}},"outputs":[{"name":"stdout","text":"Merge operation 100/2447, vocab size: 153\nMerge operation 200/2447, vocab size: 253\nMerge operation 300/2447, vocab size: 353\nMerge operation 400/2447, vocab size: 453\nMerge operation 500/2447, vocab size: 553\nMerge operation 600/2447, vocab size: 653\nMerge operation 700/2447, vocab size: 753\nMerge operation 800/2447, vocab size: 853\nMerge operation 900/2447, vocab size: 953\nMerge operation 1000/2447, vocab size: 1053\nMerge operation 1100/2447, vocab size: 1153\nMerge operation 1200/2447, vocab size: 1253\nMerge operation 1300/2447, vocab size: 1353\nMerge operation 1400/2447, vocab size: 1453\nMerge operation 1500/2447, vocab size: 1553\nMerge operation 1600/2447, vocab size: 1653\nMerge operation 1700/2447, vocab size: 1753\nMerge operation 1800/2447, vocab size: 1853\nMerge operation 1900/2447, vocab size: 1953\nMerge operation 2000/2447, vocab size: 2053\nMerge operation 2100/2447, vocab size: 2153\nMerge operation 2200/2447, vocab size: 2253\nMerge operation 2300/2447, vocab size: 2353\nMerge operation 2400/2447, vocab size: 2453\nBPE training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"bert_kan_dataset = BertPretrainingDataset(\n    texts=kanglish,\n    tokenizer=bpe_kan_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_kan_dataset)\ntrain_dataloader = DataLoader(\n    bert_kan_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"4WiXTjyEkI9d","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:23:47.623073Z","iopub.execute_input":"2025-04-22T05:23:47.623409Z","iopub.status.idle":"2025-04-22T05:26:36.426770Z","shell.execute_reply.started":"2025-04-22T05:23:47.623384Z","shell.execute_reply":"2025-04-22T05:26:36.425717Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:56<00:00,  1.90it/s, loss=7.91]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.3310\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:55<00:00,  1.94it/s, loss=6.82]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.5609\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:54<00:00,  1.97it/s, loss=7.17]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.3230\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Initialize the custom BPE tokenizer\nbpe_hin_tokenizer = BPETokenizer(vocab_size=2500, max_length=128)\nbpe_hin_tokenizer.train(hindi_corpus)\n","metadata":{"id":"4u_wNqsejRWH","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:34:29.134081Z","iopub.execute_input":"2025-04-22T03:34:29.134314Z","iopub.status.idle":"2025-04-22T03:35:09.268625Z","shell.execute_reply.started":"2025-04-22T03:34:29.134293Z","shell.execute_reply":"2025-04-22T03:35:09.267669Z"}},"outputs":[{"name":"stdout","text":"Merge operation 100/2391, vocab size: 209\nMerge operation 200/2391, vocab size: 309\nMerge operation 300/2391, vocab size: 409\nMerge operation 400/2391, vocab size: 509\nMerge operation 500/2391, vocab size: 609\nMerge operation 600/2391, vocab size: 709\nMerge operation 700/2391, vocab size: 809\nMerge operation 800/2391, vocab size: 909\nMerge operation 900/2391, vocab size: 1009\nMerge operation 1000/2391, vocab size: 1109\nMerge operation 1100/2391, vocab size: 1209\nMerge operation 1200/2391, vocab size: 1309\nMerge operation 1300/2391, vocab size: 1409\nMerge operation 1400/2391, vocab size: 1509\nMerge operation 1500/2391, vocab size: 1609\nMerge operation 1600/2391, vocab size: 1709\nMerge operation 1700/2391, vocab size: 1809\nMerge operation 1800/2391, vocab size: 1909\nMerge operation 1900/2391, vocab size: 2009\nMerge operation 2000/2391, vocab size: 2109\nMerge operation 2100/2391, vocab size: 2209\nMerge operation 2200/2391, vocab size: 2309\nMerge operation 2300/2391, vocab size: 2409\nBPE training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"bert_hin_dataset = BertPretrainingDataset(\n    texts=hinglish,\n    tokenizer=bpe_hin_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_hin_dataset)\ntrain_dataloader = DataLoader(\n    bert_hin_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"FKxDaEHakJ9D","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:26:36.428309Z","iopub.execute_input":"2025-04-22T05:26:36.428652Z","iopub.status.idle":"2025-04-22T05:31:20.701631Z","shell.execute_reply.started":"2025-04-22T05:26:36.428620Z","shell.execute_reply":"2025-04-22T05:31:20.700700Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [01:33<00:00,  1.16it/s, loss=7.92]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.1658\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [01:32<00:00,  1.17it/s, loss=7.22]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.3683\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [01:36<00:00,  1.13it/s, loss=7.63]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.1859\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Wordpiece instances","metadata":{"id":"9iGYgGbSlOOg"}},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nwp_tam_tokenizer = WordPieceTokenizer(vocab_size=2500, max_length=128)\nwp_tam_tokenizer.train(tamil_corpus)\n","metadata":{"id":"JdQ7Z9ZnlUIw","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:38:29.684956Z","iopub.execute_input":"2025-04-22T03:38:29.685272Z","iopub.status.idle":"2025-04-22T03:38:29.812981Z","shell.execute_reply.started":"2025-04-22T03:38:29.685249Z","shell.execute_reply":"2025-04-22T03:38:29.812212Z"}},"outputs":[{"name":"stdout","text":"WordPiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"bert_tam_dataset = BertPretrainingDataset(\n    texts=tanglish,\n    tokenizer=wp_tam_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_tam_dataset)\ntrain_dataloader = DataLoader(\n    bert_tam_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"ytFfPxFAmWKR","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:20.703719Z","iopub.execute_input":"2025-04-22T05:31:20.703951Z","iopub.status.idle":"2025-04-22T05:32:46.612321Z","shell.execute_reply.started":"2025-04-22T05:31:20.703928Z","shell.execute_reply":"2025-04-22T05:32:46.611422Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:28<00:00,  3.48it/s, loss=7.97]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.4637\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:27<00:00,  3.54it/s, loss=7.82]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.6975\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:28<00:00,  3.53it/s, loss=7.55]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.4368\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nwp_kan_tokenizer = WordPieceTokenizer(vocab_size=2500, max_length=128)\nwp_kan_tokenizer.train(kannada_corpus)\n","metadata":{"id":"Di5DS7Waly-9","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:39:27.942248Z","iopub.execute_input":"2025-04-22T03:39:27.942662Z","iopub.status.idle":"2025-04-22T03:39:28.063906Z","shell.execute_reply.started":"2025-04-22T03:39:27.942578Z","shell.execute_reply":"2025-04-22T03:39:28.063064Z"}},"outputs":[{"name":"stdout","text":"WordPiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"bert_kan_dataset = BertPretrainingDataset(\n    texts=kanglish,\n    tokenizer=wp_kan_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_kan_dataset)\ntrain_dataloader = DataLoader(\n    bert_kan_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"fcyFSiLNmeFR","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:32:46.613905Z","iopub.execute_input":"2025-04-22T05:32:46.614150Z","iopub.status.idle":"2025-04-22T05:34:20.013978Z","shell.execute_reply.started":"2025-04-22T05:32:46.614127Z","shell.execute_reply":"2025-04-22T05:34:20.012984Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.51it/s, loss=8.21]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.4098\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.53it/s, loss=7.34]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.6294\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.53it/s, loss=6.85]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.4229\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nwp_hin_tokenizer = WordPieceTokenizer(vocab_size=2500, max_length=128)\nwp_hin_tokenizer.train(hindi_corpus)\n","metadata":{"id":"YIgfzSTnlzzh","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:40:31.175099Z","iopub.execute_input":"2025-04-22T03:40:31.175429Z","iopub.status.idle":"2025-04-22T03:40:31.297561Z","shell.execute_reply.started":"2025-04-22T03:40:31.175403Z","shell.execute_reply":"2025-04-22T03:40:31.296882Z"}},"outputs":[{"name":"stdout","text":"WordPiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"bert_hin_dataset = BertPretrainingDataset(\n    texts=hinglish,\n    tokenizer=wp_hin_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_hin_dataset)\ntrain_dataloader = DataLoader(\n    bert_hin_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"9iKHt_HtmhlI","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:20.014977Z","iopub.execute_input":"2025-04-22T05:34:20.015277Z","iopub.status.idle":"2025-04-22T05:35:55.349504Z","shell.execute_reply.started":"2025-04-22T05:34:20.015252Z","shell.execute_reply":"2025-04-22T05:35:55.348568Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.49it/s, loss=7.5] \n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.1235\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.48it/s, loss=7.16]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.2237\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.49it/s, loss=6.64]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.0627\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Sentence Piece instances","metadata":{"id":"I0eHJBB1nNs8"}},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nsp_tam_tokenizer = SentencePieceTokenizer(vocab_size=2500, max_length=128)\nsp_tam_tokenizer.train(tamil_corpus)\n","metadata":{"id":"XU9eh2UAnS0_","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:41:35.515998Z","iopub.execute_input":"2025-04-22T03:41:35.516389Z","iopub.status.idle":"2025-04-22T03:52:20.704294Z","shell.execute_reply.started":"2025-04-22T03:41:35.516336Z","shell.execute_reply":"2025-04-22T03:52:20.703601Z"}},"outputs":[{"name":"stdout","text":"SentencePiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"bert_tam_dataset = BertPretrainingDataset(\n    texts=tanglish,\n    tokenizer=sp_tam_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_tam_dataset)\ntrain_dataloader = DataLoader(\n    bert_tam_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"o6pjlYU0nStK","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:55.350443Z","iopub.execute_input":"2025-04-22T05:35:55.350732Z","iopub.status.idle":"2025-04-22T05:37:20.960016Z","shell.execute_reply.started":"2025-04-22T05:35:55.350706Z","shell.execute_reply":"2025-04-22T05:37:20.959107Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:28<00:00,  3.53it/s, loss=8.41]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.6572\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:28<00:00,  3.53it/s, loss=7.74]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.9721\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 99/99 [00:28<00:00,  3.53it/s, loss=7.72]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.7397\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nsp_kan_tokenizer = SentencePieceTokenizer(vocab_size=2500, max_length=128)\nsp_kan_tokenizer.train(kannada_corpus)\n","metadata":{"id":"5NWUZ6EpnSlL","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:53:19.852513Z","iopub.execute_input":"2025-04-22T03:53:19.852783Z","iopub.status.idle":"2025-04-22T04:05:55.900433Z","shell.execute_reply.started":"2025-04-22T03:53:19.852759Z","shell.execute_reply":"2025-04-22T04:05:55.899603Z"}},"outputs":[{"name":"stdout","text":"SentencePiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"bert_kan_dataset = BertPretrainingDataset(\n    texts=kanglish,\n    tokenizer=sp_kan_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_kan_dataset)\ntrain_dataloader = DataLoader(\n    bert_kan_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"-uSHhBg8nSe_","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:37:20.961086Z","iopub.execute_input":"2025-04-22T05:37:20.961343Z","iopub.status.idle":"2025-04-22T05:38:54.227465Z","shell.execute_reply.started":"2025-04-22T05:37:20.961320Z","shell.execute_reply":"2025-04-22T05:38:54.226537Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.53it/s, loss=8.13]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.5295\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.53it/s, loss=7.57]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.9325\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 108/108 [00:30<00:00,  3.54it/s, loss=7.6] ","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.7356\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Initialize the custom word piece tokenizer\nsp_hin_tokenizer = SentencePieceTokenizer(vocab_size=2500, max_length=128)\nsp_hin_tokenizer.train(hindi_corpus)\n","metadata":{"id":"cKGdBvKunSWF","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:06:59.818053Z","iopub.execute_input":"2025-04-22T04:06:59.818306Z","iopub.status.idle":"2025-04-22T04:32:13.905855Z","shell.execute_reply.started":"2025-04-22T04:06:59.818283Z","shell.execute_reply":"2025-04-22T04:32:13.904874Z"}},"outputs":[{"name":"stdout","text":"SentencePiece training complete. Final vocabulary size: 2500\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"bert_hin_dataset = BertPretrainingDataset(\n    texts=hinglish,\n    tokenizer=sp_hin_tokenizer,\n    max_length=128,\n    mlm_probability=0.15\n)\n\ntrain_sampler = RandomSampler(bert_hin_dataset)\ntrain_dataloader = DataLoader(\n    bert_hin_dataset,\n    sampler=train_sampler,\n    batch_size=16,  # Reduced batch size for Colab\n    num_workers=2\n)\n\n# Initialize the BERT model\nconfig = BertConfig()\nmodel = BertForPreTraining(config)\nmodel = model.to(device)\n\n# Print model architecture\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n\n# Training parameters\nnum_epochs = 3  # For demonstration, increase for better results\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\n\n# Learning rate scheduler\nfrom transformers import get_linear_schedule_with_warmup\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Main training function\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    avg_loss = train()\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"ix3CpQGao9k3","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:38:54.229878Z","iopub.execute_input":"2025-04-22T05:38:54.230137Z","iopub.status.idle":"2025-04-22T05:40:29.578073Z","shell.execute_reply.started":"2025-04-22T05:38:54.230106Z","shell.execute_reply":"2025-04-22T05:40:29.577075Z"}},"outputs":[{"name":"stdout","text":"Model Parameters: 67579196\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.49it/s, loss=7.89]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 9.5183\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.48it/s, loss=7.49]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.7196\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 109/109 [00:31<00:00,  3.48it/s, loss=7.76]","output_type":"stream"},{"name":"stdout","text":"Average loss: 7.5617\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":36}]}